{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":27297,"status":"ok","timestamp":1729519685461,"user":{"displayName":"Matteo Aprile","userId":"01782337209846835788"},"user_tz":-120},"id":"CfQ4iqpUYV-s"},"outputs":[],"source":["import os\n","import cv2\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset, random_split\n","import torchvision.transforms as transforms\n","import random\n","from sklearn.model_selection import train_test_split\n","import pickle"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1729519685461,"user":{"displayName":"Matteo Aprile","userId":"01782337209846835788"},"user_tz":-120},"id":"36-RYQl7YXC4"},"outputs":[],"source":["### Constants\n","DATA_PATH = \"./train_data\" # path to the dataset\n","LABEL_PATH = './media/labels.csv' # path to the labels csv file\n","RESULT_PATH = \"./model_trained.p\" # path to the labels csv file\n","BATCH_SIZE = 50  # size of batches during training\n","IMG_DIM = (32, 32)  # dimensions of the images (height, width)\n","TEST_SPLIT = 0.2  # proportion of data used for testing\n","VAL_SPLIT = 0.2  # proportion of training data for validation\n","N_EPOCHS = 30  # number of epochs for training"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1729519685461,"user":{"displayName":"Matteo Aprile","userId":"01782337209846835788"},"user_tz":-120},"id":"YCZtMWC9YdOK"},"outputs":[],"source":["### Custom Dataset\n","class TrafficSignsDataset(Dataset):\n","    def __init__(self, data_path, transform=None):\n","        self.data_path = data_path\n","        self.classes = os.listdir(data_path)\n","        self.images = []\n","        self.labels = []\n","        self.transform = transform\n","        self.load_data()\n","\n","    def load_data(self):\n","        for class_id, class_name in enumerate(self.classes):\n","            class_folder = os.path.join(self.data_path, class_name)\n","            if not os.path.isdir(class_folder):\n","                continue\n","            for image_name in os.listdir(class_folder):\n","                if image_name.startswith('.'):\n","                    continue\n","                image_path = os.path.join(class_folder, image_name)\n","                img = cv2.imread(image_path)\n","                if img is not None:\n","                    img = cv2.resize(img, IMG_DIM)\n","                    self.images.append(img)\n","                    self.labels.append(class_id)\n","\n","        self.images = np.array(self.images)\n","        self.labels = np.array(self.labels)\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        img = self.images[idx]\n","        label = self.labels[idx]\n","\n","        if self.transform:\n","            img = self.transform(img)\n","\n","        return img, label"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["class TrafficSignModel(nn.Module):\n","    def __init__(self, n_classes, n_kernels=60, n_nodes=500):\n","        super(TrafficSignModel, self).__init__()\n","\n","        # Define kernel sizes and pool size\n","        kernel_size_5x5 = (5, 5)\n","        kernel_size_3x3 = (3, 3)\n","        pool_size = (2, 2)\n","        \n","        # First convolutional block\n","        self.conv1 = nn.Conv2d(in_channels=1, out_channels=n_kernels, kernel_size=kernel_size_5x5)\n","        self.conv2 = nn.Conv2d(in_channels=n_kernels, out_channels=n_kernels, kernel_size=kernel_size_5x5)\n","        self.pool = nn.MaxPool2d(kernel_size=pool_size)\n","\n","        # Second convolutional block\n","        self.conv3 = nn.Conv2d(in_channels=n_kernels, out_channels=n_kernels // 2, kernel_size=kernel_size_3x3)\n","        self.conv4 = nn.Conv2d(in_channels=n_kernels // 2, out_channels=n_kernels // 2, kernel_size=kernel_size_3x3)\n","\n","        # Dropout layer\n","        self.dropout = nn.Dropout(0.5)\n","\n","        # Fully connected layers\n","        conv_output_size = self._get_conv_output_size()\n","        self.fc1 = nn.Linear(conv_output_size, n_nodes)\n","        self.fc2 = nn.Linear(n_nodes, n_classes)\n","\n","    def _get_conv_output_size(self):\n","        # Pass a dummy input through the conv layers to calculate the output size\n","        with torch.no_grad():\n","            dummy_input = torch.zeros(1, 1, IMG_DIM[0], IMG_DIM[1])\n","            x = nn.functional.relu(self.conv1(dummy_input))\n","            x = nn.functional.relu(self.conv2(x))\n","            x = self.pool(x)\n","\n","            x = nn.functional.relu(self.conv3(x))\n","            x = nn.functional.relu(self.conv4(x))\n","            x = self.pool(x)\n","\n","            output_size = x.numel()  # Flatten size\n","        return output_size\n","\n","    def forward(self, x):\n","        # First conv block\n","        x = nn.functional.relu(self.conv1(x))\n","        x = nn.functional.relu(self.conv2(x))\n","        x = self.pool(x)\n","\n","        # Second conv block\n","        x = nn.functional.relu(self.conv3(x))\n","        x = nn.functional.relu(self.conv4(x))\n","        x = self.pool(x)\n","        x = self.dropout(x)\n","\n","        # Flatten the output\n","        x = torch.flatten(x, 1)\n","\n","        # Fully connected layers\n","        x = nn.functional.relu(self.fc1(x))\n","        x = self.dropout(x)\n","        x = self.fc2(x)\n","\n","        # Output with softmax activation\n","        return nn.functional.softmax(x, dim=1)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"background_save":true},"id":"bmkURGeKYjUj"},"outputs":[],"source":["# Load the dataset\n","transform = transforms.Compose([\n","    transforms.ToPILImage(),\n","    transforms.Grayscale(),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize grayscale images\n","])\n","\n","dataset = TrafficSignsDataset(DATA_PATH, transform=transform)\n","n_classes = len(dataset.classes)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"background_save":true},"id":"JSTmZ058Ylue"},"outputs":[],"source":["# Split dataset into train, val, and test sets\n","test_size = int(TEST_SPLIT * len(dataset))\n","val_size = int(VAL_SPLIT * (len(dataset) - test_size))\n","train_size = len(dataset) - test_size - val_size\n","train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n","\n","# Create DataLoader for batching\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"background_save":true},"id":"4VpsXas4Yo7q"},"outputs":[],"source":["# Initialize model, loss function, and optimizer\n","model = TrafficSignModel(n_classes)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)"]},{"cell_type":"code","execution_count":68,"metadata":{"colab":{"background_save":true},"id":"AblxxqfrYrdH"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/30, Train Loss: 3.4324, Train Accuracy: 0.2495, Val Loss: 3.3438, Val Accuracy: 0.3390\n","Epoch 2/30, Train Loss: 3.3308, Train Accuracy: 0.3512, Val Loss: 3.2801, Val Accuracy: 0.4015\n","Epoch 3/30, Train Loss: 3.2892, Train Accuracy: 0.3926, Val Loss: 3.2630, Val Accuracy: 0.4190\n","Epoch 4/30, Train Loss: 3.2731, Train Accuracy: 0.4087, Val Loss: 3.2387, Val Accuracy: 0.4434\n","Epoch 5/30, Train Loss: 3.2511, Train Accuracy: 0.4310, Val Loss: 3.2323, Val Accuracy: 0.4489\n","Epoch 6/30, Train Loss: 3.2356, Train Accuracy: 0.4463, Val Loss: 3.2078, Val Accuracy: 0.4738\n","Epoch 7/30, Train Loss: 3.2258, Train Accuracy: 0.4559, Val Loss: 3.2082, Val Accuracy: 0.4739\n","Epoch 8/30, Train Loss: 3.2193, Train Accuracy: 0.4625, Val Loss: 3.2051, Val Accuracy: 0.4765\n","Epoch 9/30, Train Loss: 3.2145, Train Accuracy: 0.4671, Val Loss: 3.1940, Val Accuracy: 0.4878\n","Epoch 10/30, Train Loss: 3.2069, Train Accuracy: 0.4745, Val Loss: 3.1775, Val Accuracy: 0.5046\n","Epoch 11/30, Train Loss: 3.1956, Train Accuracy: 0.4863, Val Loss: 3.1719, Val Accuracy: 0.5100\n","Epoch 12/30, Train Loss: 3.1919, Train Accuracy: 0.4897, Val Loss: 3.1682, Val Accuracy: 0.5134\n","Epoch 13/30, Train Loss: 3.1847, Train Accuracy: 0.4969, Val Loss: 3.1734, Val Accuracy: 0.5083\n","Epoch 14/30, Train Loss: 3.1914, Train Accuracy: 0.4904, Val Loss: 3.1722, Val Accuracy: 0.5096\n","Epoch 15/30, Train Loss: 3.1807, Train Accuracy: 0.5008, Val Loss: 3.1523, Val Accuracy: 0.5298\n","Epoch 16/30, Train Loss: 3.1635, Train Accuracy: 0.5178, Val Loss: 3.1432, Val Accuracy: 0.5383\n","Epoch 17/30, Train Loss: 3.1639, Train Accuracy: 0.5179, Val Loss: 3.1401, Val Accuracy: 0.5418\n","Epoch 18/30, Train Loss: 3.1595, Train Accuracy: 0.5221, Val Loss: 3.1417, Val Accuracy: 0.5401\n","Epoch 19/30, Train Loss: 3.1593, Train Accuracy: 0.5225, Val Loss: 3.1383, Val Accuracy: 0.5435\n","Epoch 20/30, Train Loss: 3.1631, Train Accuracy: 0.5186, Val Loss: 3.1444, Val Accuracy: 0.5375\n","Epoch 21/30, Train Loss: 3.1635, Train Accuracy: 0.5181, Val Loss: 3.1525, Val Accuracy: 0.5295\n","Epoch 22/30, Train Loss: 3.1581, Train Accuracy: 0.5237, Val Loss: 3.1388, Val Accuracy: 0.5431\n","Epoch 23/30, Train Loss: 3.1584, Train Accuracy: 0.5235, Val Loss: 3.1402, Val Accuracy: 0.5417\n","Epoch 24/30, Train Loss: 3.1570, Train Accuracy: 0.5247, Val Loss: 3.1406, Val Accuracy: 0.5414\n","Epoch 25/30, Train Loss: 3.1623, Train Accuracy: 0.5198, Val Loss: 3.1463, Val Accuracy: 0.5353\n","Epoch 26/30, Train Loss: 3.1610, Train Accuracy: 0.5206, Val Loss: 3.1353, Val Accuracy: 0.5465\n","Epoch 27/30, Train Loss: 3.1612, Train Accuracy: 0.5206, Val Loss: 3.1417, Val Accuracy: 0.5403\n","Epoch 28/30, Train Loss: 3.1614, Train Accuracy: 0.5204, Val Loss: 3.1369, Val Accuracy: 0.5449\n","Epoch 29/30, Train Loss: 3.1689, Train Accuracy: 0.5125, Val Loss: 3.1488, Val Accuracy: 0.5329\n","Epoch 30/30, Train Loss: 3.1662, Train Accuracy: 0.5159, Val Loss: 3.1387, Val Accuracy: 0.5431\n"]}],"source":["# Training loop\n","for epoch in range(N_EPOCHS):\n","    model.train()\n","    running_loss = 0.0\n","    correct_train = 0\n","\n","    for images, labels in train_loader:\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","        correct_train += (outputs.argmax(dim=1) == labels).sum().item()\n","\n","    # Validation\n","    model.eval()\n","    correct_val = 0\n","    val_loss = 0.0\n","    with torch.no_grad():\n","        for images, labels in val_loader:\n","            outputs = model(images)\n","            val_loss += criterion(outputs, labels).item()\n","            correct_val += (outputs.argmax(dim=1) == labels).sum().item()\n","\n","    # Print epoch results\n","    print(f\"Epoch {epoch + 1}/{N_EPOCHS}, \"\n","          f\"Train Loss: {running_loss / len(train_loader):.4f}, \"\n","          f\"Train Accuracy: {correct_train / len(train_loader.dataset):.4f}, \"\n","          f\"Val Loss: {val_loss / len(val_loader):.4f}, \"\n","          f\"Val Accuracy: {correct_val / len(val_loader.dataset):.4f}\")"]},{"cell_type":"code","execution_count":71,"metadata":{"colab":{"background_save":true},"id":"q_ALPs8JYSKP"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Loss: 3.1399, Test Accuracy: 0.5419\n"]}],"source":["# Testing\n","model.eval()\n","correct_test = 0\n","test_loss = 0.0\n","with torch.no_grad():\n","    for images, labels in test_loader:\n","        outputs = model(images)\n","        test_loss += criterion(outputs, labels).item()\n","        correct_test += (outputs.argmax(dim=1) == labels).sum().item()\n","\n","print(f\"Test Loss: {test_loss / len(test_loader):.4f}, \"\n","      f\"Test Accuracy: {correct_test / len(test_loader.dataset):.4f}\")\n","\n","# save the trained model\n","pickle_out = open(RESULT_PATH, \"wb\")\n","pickle.dump(model, pickle_out) # serialize and save the model\n","pickle_out.close()"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyP9Wqw6eZwYJqeNOnx3TJYs","mount_file_id":"1uwKU63Uv3Dg55g7EMT2arP5UoJKUmkwf","name":"","version":""},"kernelspec":{"display_name":".env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"}},"nbformat":4,"nbformat_minor":0}
